# Transformer

##	Word Embedding(词嵌入)
+ WTE(Word Embedding Matrix，词嵌入矩阵)
+ shape：假设WTE的维度是d_model，词汇表中有N个单词或标记。那么WTE矩阵的形状将是(N, d_model)。
  + N：词汇表中的单词或标记数量。
  + d_model：每个单词或标记的词嵌入向量的维度。
+ 当模型处理输入序列时，每个单词或标记会被映射到WTE矩阵中的一个行向量，随后向量被输入到模型的其他部分，如位置编码、编码器层等。
  
## Multi-Head Attention(多头注意力)
+ 编码过的token向量进入自注意力(Self-Attention)层。
+ 在自注意力层中，每个token的向量会被分割并输入到多个注意力头中。(如模型有12个头，768维的向量将被分割成12份，每份的维度是768/12 = 64维。这意味着每个头将处理64维的向量。)
+ 每个注意力头都各有3个权重矩阵(作为需要学习的参数)
  + W_Q(查询权重矩阵)：用于将输入向量转换为查询表示，这将用于计算对其他token的注意力分数。
  + W_K(键权重矩阵)：用于将输入向量转换为键表示，与查询一起参与计算注意力分数。
  + W_V(值权重矩阵)：用于将输入向量转换为值表示，这些值将根据计算出的注意力分数进行加权求和。
+ 对于序列中的每个token，其d_model维度的向量分别与这三个权重矩阵相乘，生成对应的Q、K、V向量
  + Q = 输入向量 × W_Q
  + K = 输入向量 × W_K
  + V = 输入向量 × W_V
+ 其大小通常与模型的维度有关：输入向量的维度：[1, d_model]，权重矩阵的维度：[d_model, d_k]，输出向量的维度：[1, d_k]（行向量）
+ 例如，如果d_model是768，而模型有12个头，那么每个矩阵的大小可能是(768, 64)，因为每个头处理的是d_model / 12的维度。
+ 然后，Q、K、V向量被拼接并输入到一个注意力函数中，该函数会计算每个token对其他所有token的注意力权重。
+ 注意力函数的输出是一个d_model维度的向量，该向量代表了该token对输入序列的注意力。
+ 最后，注意力函数的输出被拼接到V向量上，并与输入向量相乘，得到新的输出向量。
+ 多头注意力层的输出是所有注意力头的输出向量的拼接。
+ 这部分的本质是一种 `带参数的注意力汇聚` 
  + `多头`：用来实现关注多个部分
  + `自`：k = v


# 其它
## 注意力机制
+ [注意力汇聚](https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)
+ [多头注意力](https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)