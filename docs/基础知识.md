# 1. NLP

## 1.1 Embedding
tokenize：将文本分割成词或标记(一般同时还会对应到idx)。
embedding：将词或标记向量表示映射到低维空间，使得相似的词或标记在低维空间中距离更近。pytorch中的nn.Embedding层其实是建立了idx到embedding向量的映射。


# 2. 注意力机制

## 2.1 双组件(two-component)框架
19世纪90年代的"美国心理学之父"威廉·詹姆斯
+ 自主性提示
+ 非自主性提示
 
## 2.2 注意力汇聚(Attention Pooling)
+ 注意力汇聚：查询(自主提示)和键(非自主提示)之间的交互形成了注意力汇聚； 注意力汇聚有选择地聚合了值(感官输入)以生成最终的输出。
+ 注意力评分函数(attention scoring function)
  + 注意力评分函数f(q,k)
  + softmax
  + 对v进行加权平均：v * softmax(f(q,k))
+ 平均汇聚
+ Nadaraya-Watson核回归：非参数模型，将查询和键之间的关系建模为注意力权重，注意力汇聚是值的加权平均。
+ 带参数的注意力汇聚
  + 加性注意力：q,k的维度可以不同
  + scaled dot-product attention*缩放点积注意力*：要示q,k的维度相同

## 2.3 Bahdanau注意力
+ 没有严格单向对齐限制的可微注意力模型

## 2.4 多头注意力
+ 允许注意力机制组合使用查询、键和值的不同子空间表示(representation subspaces)
+ 可以用独立学习得到的h组不同的线性投影(linear projections)来变换查询、键和值，然后这h组变换后的查询、键和值将并行地送到注意力汇聚中，最后将这h个注意力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。
+ h个头有各自的一套可学习参数(Wk, Wq, Wv, W_proj)

## 2.5 自注意力
+ 由于查询、键和值来自同一组输入，因此被称为`自注意力(self-attention)`
+ 每个词元跟同一组输入中的其它词元都进行注意力计算，并生成输出。
+ 自注意力有并行计算的优势，并且最大路径长度最短。

## 2.6 顺序信息
+ 为了使用序列的顺序信息，可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。




# 3. Pytorch基础
## 结构
+ torch.tensor
+ torch.autograd
+ torch.utils
  + datasets
  + data
  + tensorboard
  + model_zoo
+ torch.nn
  + Module
  + functional
+ torch.optim
  + Adam
  + AdamW
  + ...




# 4. Transformer基础

## 输入与输出
+ 输入：一个序列，表示多个token的idx
+ 输出：一个vocab_size维度的向量，表示下一个token在词汇表中的概率分布。

##	Word Embedding(词嵌入)
+ WTE(Word Embedding Matrix，词嵌入矩阵)
+ shape：假设WTE的维度是d_model，词汇表中有N个单词或标记。那么WTE矩阵的形状将是(N, d_model)。
  + N：词汇表中的单词或标记数量。
  + d_model：每个单词或标记的词嵌入向量的维度。
+ 当模型处理输入序列时，每个单词或标记会被映射到WTE矩阵中的一个行向量，随后向量被输入到模型的其他部分，如位置编码、编码器层等。

## Position Embedding(位置嵌入)
+ PE(Position Encoding Matrix，位置编码矩阵)
+ shape：假设PE的维度是d_model，输入序列的长度是L。那么PE矩阵的形状将是(L, d_model)。
  + L：输入序列的长度。
  + d_model：每个位置的词嵌入向量的维度。
+ 位置编码是一种常用的技术，可以帮助模型学习到序列中不同位置的特征。
+ 位置编码向量的每一行对应于输入序列中的一个位置，其中的元素是该位置的相对位置编码。
+ 函数：正弦余弦函数（Sinusoidal Functions）
+ 这个值只与Position有关与token无关，不需要训练。
+ 允许模型学习得到输入序列中绝对和相对位置信息：因为对于任何确定的位置偏移$\delta$，位置$i+\delta$处的位置编码可以线性投影位置$i$处的位置编码来表示。
  
## Multi-Head Self-Attention(多头自注意力)
+ 编码过的token向量进入自注意力(Self-Attention)层。
+ 在自注意力层中，每个token的向量会被分割并输入到多个注意力头中。(如模型有12个头，768维的向量将被分割成12份，每份的维度是768/12 = 64维。这意味着每个头将处理64维的向量。)
+ 每个注意力头都各有3个权重矩阵(作为需要学习的参数)
  + W_Q(查询权重矩阵)：用于将输入向量转换为查询表示，这将用于计算对其他token的注意力分数。
  + W_K(键权重矩阵)：用于将输入向量转换为键表示，与查询一起参与计算注意力分数。
  + W_V(值权重矩阵)：用于将输入向量转换为值表示，这些值将根据计算出的注意力分数进行加权求和。
+ 对于序列中的每个token，其d_model维度的向量分别与这三个权重矩阵相乘，生成对应的Q、K、V向量
  + Q = 输入向量 × W_Q
  + K = 输入向量 × W_K
  + V = 输入向量 × W_V
+ 其大小通常与模型的维度有关：输入向量的维度：[1, d_model]，权重矩阵的维度：[d_model, d_k]，输出向量的维度：[1, d_k]（行向量）
+ 例如，如果d_model是768，而模型有12个头，那么每个矩阵的大小可能是(768, 64)，因为每个头处理的是d_model / 12的维度。
<!-- + 然后，Q、K向量被拼接并输入到一个注意力函数中，注意力函数的输出被拼接到V向量上，该函数会计算每个token对其他所有token的注意力权重。 -->
+ 注意力函数的输出是一个d_model维度的向量，该向量代表了该token对输入序列的注意力。
+ 多头注意力层的输出是所有注意力头的输出向量的拼接。
+ 这部分的本质是一种 `带参数的注意力汇聚` 
  + `多头`：用来实现关注多个部分
  + `自`：q, k, v来自同一组输入
+ 示例：
  + 有个序列L, 共含有n个token(i=1,2,...,n)；有可学习的参数矩阵Wq, Wk, Wv, W0。
  + 以计算L1的注意力attention_1为例：
    + 先令 Qi = Li × Wq, Ki = Li × Wk, Vi = Li × Wv
    + 再令 att = softmax(f(Q1, Ki)) * Vi
    + 最后得到 attention_1 = W0 * att


# 5. GPT-2知识

## 5.1 参考
[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
## 5.2 model type
| Parameters| Layers | d_model | hf_name|
|---|---|---|---|
| 117M|  12| 768|  gpt2|
| 345M|  24| 1024| gpt2-medium|
| 762M|  36| 1280| gpt2-large|
| 1542M| 48| 1600| gpt2-xl|

## 5.3 结构
+ Embedding
  + WTE(Token Embedding) + WPE(Position Embedding)
+ Block * 12
    + Layer Normalization
    + CasualSelfAttention
    + Residual
    + Layer Normalization
    + MLP
        + Linear 
        + GELU 
        + Linear
    + Residual
+ Layer Normalization
+ Head

## 5.4 GPT-2的实现与Transformer架构的不同
GPT-2的实际实现与"Attention Is All You Need"论文中Transformer框架有以下不同：
+ 只有Decoder部分
+ 调整了Layer Normalization的位置：放在attention layer之前
+ 在最后的添加了一个额外的Layer Normalization

# 6. 疑问
## 在Transfomer计算loss时，为什么可以直接用y?

## 当输入的长度小于block_size时，Transfomer是怎么处理的？


# 其它
## 注意力机制
+ [注意力汇聚](https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html)
+ [多头注意力](https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html)